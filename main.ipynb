{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4558742,"sourceType":"datasetVersion","datasetId":2660745}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport os\nfrom tensorflow.keras import layers\nimport string\nfrom IPython.display import clear_output","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:35.947790Z","iopub.execute_input":"2025-11-12T05:32:35.948034Z","iopub.status.idle":"2025-11-12T05:32:48.904346Z","shell.execute_reply.started":"2025-11-12T05:32:35.948011Z","shell.execute_reply":"2025-11-12T05:32:48.903764Z"}},"outputs":[{"name":"stderr","text":"2025-11-12 05:32:37.557111: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762925557.747910      39 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762925557.798173      39 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"BATCH_SIZE = 128\nNUM_HEADS = 12\nNUM_BLOCKS = 2\nEMBED_DIM = 384\nDENSE_DIM = 1536\nDROPOUT_RATE = 0.3\nCHUNK_LENGTH = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:48.905989Z","iopub.execute_input":"2025-11-12T05:32:48.906482Z","iopub.status.idle":"2025-11-12T05:32:48.910485Z","shell.execute_reply.started":"2025-11-12T05:32:48.906455Z","shell.execute_reply":"2025-11-12T05:32:48.909743Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/the-bards-best-a-character-modeling-dataset/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:48.913673Z","iopub.execute_input":"2025-11-12T05:32:48.913913Z","iopub.status.idle":"2025-11-12T05:32:49.123015Z","shell.execute_reply.started":"2025-11-12T05:32:48.913892Z","shell.execute_reply":"2025-11-12T05:32:49.122476Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"text = df.values[0][0]\ntext = re.sub(r'\\s+', ' ', str(text)).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:49.123640Z","iopub.execute_input":"2025-11-12T05:32:49.123834Z","iopub.status.idle":"2025-11-12T05:32:49.192548Z","shell.execute_reply.started":"2025-11-12T05:32:49.123818Z","shell.execute_reply":"2025-11-12T05:32:49.191832Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import re\n\nwords = text.split()\n\n# Count unique words\nunique_words = set(words)\nprint(f\"Total words: {len(words)}\")\nprint(f\"Unique words: {len(unique_words)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:49.193291Z","iopub.execute_input":"2025-11-12T05:32:49.193925Z","iopub.status.idle":"2025-11-12T05:32:49.232072Z","shell.execute_reply.started":"2025-11-12T05:32:49.193906Z","shell.execute_reply":"2025-11-12T05:32:49.231427Z"}},"outputs":[{"name":"stdout","text":"Total words: 182499\nUnique words: 23841\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def chunk_text_by_words(text, max_words, stride=None):\n    words = text.split()\n    if stride is None:\n        stride = max_words // 2\n    chunks = []\n    for i in range(0, len(words) - max_words, stride):\n        chunk = ' '.join(words[i:i + max_words])\n        chunks.append(chunk)\n    return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:49.234153Z","iopub.execute_input":"2025-11-12T05:32:49.234388Z","iopub.status.idle":"2025-11-12T05:32:49.246653Z","shell.execute_reply.started":"2025-11-12T05:32:49.234345Z","shell.execute_reply":"2025-11-12T05:32:49.245900Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"chunks = chunk_text_by_words(text, CHUNK_LENGTH+1, 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:49.247213Z","iopub.execute_input":"2025-11-12T05:32:49.247407Z","iopub.status.idle":"2025-11-12T05:32:49.307649Z","shell.execute_reply.started":"2025-11-12T05:32:49.247386Z","shell.execute_reply":"2025-11-12T05:32:49.306899Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(np.shape(chunks))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:49.308412Z","iopub.execute_input":"2025-11-12T05:32:49.308614Z","iopub.status.idle":"2025-11-12T05:32:49.354540Z","shell.execute_reply.started":"2025-11-12T05:32:49.308597Z","shell.execute_reply":"2025-11-12T05:32:49.353899Z"}},"outputs":[{"name":"stdout","text":"(6075,)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"strip_chars = string.punctuation + \"¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(\n        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n\nvocab_size = 12050\nsequence_length = CHUNK_LENGTH+1\n\nvectorizer = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n    standardize=custom_standardization\n)\n\nvectorizer.adapt(chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:49.355301Z","iopub.execute_input":"2025-11-12T05:32:49.355763Z","iopub.status.idle":"2025-11-12T05:32:50.565682Z","shell.execute_reply.started":"2025-11-12T05:32:49.355741Z","shell.execute_reply":"2025-11-12T05:32:50.565045Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1762925569.976860      39 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"vocab = vectorizer.get_vocabulary()\nprint(\"Total unique tokens in vocabulary:\", len(vocab))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:50.566405Z","iopub.execute_input":"2025-11-12T05:32:50.566640Z","iopub.status.idle":"2025-11-12T05:32:50.594940Z","shell.execute_reply.started":"2025-11-12T05:32:50.566623Z","shell.execute_reply":"2025-11-12T05:32:50.594209Z"}},"outputs":[{"name":"stdout","text":"Total unique tokens in vocabulary: 12050\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def make_dataset(chunks):\n    tokens = vectorizer(chunks)\n    tokens_inp = tokens[:,:CHUNK_LENGTH]\n    tokens_out = tokens[:,1:]\n    ds = tf.data.Dataset.from_tensor_slices((tokens_inp,tokens_out))\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.shuffle(1024).prefetch(16).cache()\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:50.595791Z","iopub.execute_input":"2025-11-12T05:32:50.596125Z","iopub.status.idle":"2025-11-12T05:32:50.600171Z","shell.execute_reply.started":"2025-11-12T05:32:50.596102Z","shell.execute_reply":"2025-11-12T05:32:50.599490Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"ds = make_dataset(chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:50.600904Z","iopub.execute_input":"2025-11-12T05:32:50.601135Z","iopub.status.idle":"2025-11-12T05:32:52.165548Z","shell.execute_reply.started":"2025-11-12T05:32:50.601114Z","shell.execute_reply":"2025-11-12T05:32:52.164980Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class PositionalEmbedding(tf.keras.layers.Layer):\n    def __init__(self, sequence_length, vocab_size, output_dim):\n        super().__init__()\n        self.positional_embedding = tf.keras.layers.Embedding(input_dim = sequence_length, output_dim = output_dim, mask_zero=False)\n        self.token_embedding = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim= output_dim, mask_zero=True)\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embedding(inputs)\n        embedded_positions = self.positional_embedding(positions)\n        return embedded_tokens + embedded_positions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:52.166317Z","iopub.execute_input":"2025-11-12T05:32:52.166834Z","iopub.status.idle":"2025-11-12T05:32:52.171820Z","shell.execute_reply.started":"2025-11-12T05:32:52.166812Z","shell.execute_reply":"2025-11-12T05:32:52.171036Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class TransformerDecoder(tf.keras.layers.Layer):\n    def __init__(self, num_heads, embed_dim, dense_dim, dropout_rate):\n        super().__init__()\n        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,\n                                                           key_dim=embed_dim//num_heads)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n        self.dense_proj = tf.keras.models.Sequential([\n            tf.keras.layers.Dense(dense_dim, activation='relu'),\n            tf.keras.layers.Dense(embed_dim)\n        ])\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n    def call(self, inputs):\n        attn_out = self.attention(query=inputs,\n                            key=inputs,\n                            value=inputs,\n                            use_causal_mask=True)\n        norm1_out = self.layernorm1(attn_out+inputs)\n        drop1_out = self.dropout1(norm1_out)\n        dense_proj_out = self.dense_proj(drop1_out)\n        norm2_out = self.layernorm2(drop1_out+dense_proj_out)\n        drop2_out = self.dropout2(norm2_out)\n        return drop2_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:52.172448Z","iopub.execute_input":"2025-11-12T05:32:52.173106Z","iopub.status.idle":"2025-11-12T05:32:52.187141Z","shell.execute_reply.started":"2025-11-12T05:32:52.173082Z","shell.execute_reply":"2025-11-12T05:32:52.186462Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"inputs = tf.keras.layers.Input(shape=(None,))\nembeddings = PositionalEmbedding(sequence_length, vocab_size, EMBED_DIM)(inputs)\nx = embeddings\nfor layer in range(NUM_BLOCKS):\n    x = TransformerDecoder(NUM_HEADS, EMBED_DIM, DENSE_DIM, DROPOUT_RATE)(x)\nx = tf.keras.layers.Dropout(0.3)(x)\noutput = tf.keras.layers.Dense(vocab_size, activation='linear', kernel_initializer='glorot_uniform')(x)\ntransformer = tf.keras.models.Model(inputs, output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:52.187919Z","iopub.execute_input":"2025-11-12T05:32:52.188195Z","iopub.status.idle":"2025-11-12T05:32:53.510966Z","shell.execute_reply.started":"2025-11-12T05:32:52.188174Z","shell.execute_reply":"2025-11-12T05:32:53.510334Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"transformer.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:53.511682Z","iopub.execute_input":"2025-11-12T05:32:53.511905Z","iopub.status.idle":"2025-11-12T05:32:53.529762Z","shell.execute_reply.started":"2025-11-12T05:32:53.511887Z","shell.execute_reply":"2025-11-12T05:32:53.529077Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ positional_embedding            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │     \u001b[38;5;34m4,725,888\u001b[0m │\n│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)           │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │     \u001b[38;5;34m1,774,464\u001b[0m │\n│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_decoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │     \u001b[38;5;34m1,774,464\u001b[0m │\n│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12050\u001b[0m)    │     \u001b[38;5;34m4,639,250\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ positional_embedding            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,725,888</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)           │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,774,464</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_decoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,774,464</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12050</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,639,250</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,914,066\u001b[0m (49.26 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,914,066</span> (49.26 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,914,066\u001b[0m (49.26 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,914,066</span> (49.26 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n\ndef sample_with_temperature(preds, temperature=1.0):\n    preds = np.asarray(preds).astype(\"float64\")\n    preds = np.log(preds + 1e-9) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    return np.random.choice(len(preds), p=preds)\n    \ndef generate_text(prompt, max_length=50):\n    for _ in range(max_length):\n        tokenized = vectorizer([prompt])\n        preds = transformer(tokenized, training=False)\n        next_id = tf.argmax(preds[0, len(prompt.split()) + 1, :]).numpy()\n        next_word = vectorizer.get_vocabulary()[next_id]\n        prompt += \" \" + next_word\n        if next_id == 0:\n            break\n    return prompt\n\ntext = generate_text('thou shall')\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:53.530576Z","iopub.execute_input":"2025-11-12T05:32:53.530775Z","iopub.status.idle":"2025-11-12T05:32:59.504541Z","shell.execute_reply.started":"2025-11-12T05:32:53.530760Z","shell.execute_reply":"2025-11-12T05:32:59.503762Z"}},"outputs":[{"name":"stdout","text":"thou shall weepst rail elbow servingman fretting would would birthan bows writing whiteliverd would preferrst ripening vincetinos appealld auroras effuse would ribs fawns dauntless federary marchd briefly unaccustomd truth hardly paper brandon hoarding birthan dulcet sweetheart moons ripening hungerford ripening bug complete haver federary lethargy dauntless lethargy fondness shelves uncleanness oermatchd errand\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class TextGeneration(tf.keras.callbacks.Callback):\n    def __init__(self, text):\n        super().__init__()\n        self.text = text\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch+1) % 50 == 0:\n            clear_output(wait=True)\n            output = generate_text(self.text)\n            print(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:59.505310Z","iopub.execute_input":"2025-11-12T05:32:59.506009Z","iopub.status.idle":"2025-11-12T05:32:59.510005Z","shell.execute_reply.started":"2025-11-12T05:32:59.505984Z","shell.execute_reply":"2025-11-12T05:32:59.509426Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nopt = tf.keras.optimizers.Adam(learning_rate=5e-4)\n\ndef perplexity(y_true, y_pred):\n    cross_entropy = K.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n    return K.exp(K.mean(cross_entropy))\ncallback = TextGeneration('to be or not to be')\ntransformer.compile(loss = loss_fn,\n                    metrics = ['accuracy', perplexity],\n                    optimizer=opt)\nhistory = transformer.fit(ds,callbacks = [callback], epochs = 400) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:32:59.510736Z","iopub.execute_input":"2025-11-12T05:32:59.511479Z","iopub.status.idle":"2025-11-12T07:02:43.900859Z","shell.execute_reply.started":"2025-11-12T05:32:59.511455Z","shell.execute_reply":"2025-11-12T07:02:43.900273Z"}},"outputs":[{"name":"stdout","text":"to be or not to be the how that with nor so it and the exeter should not it it have the i when is this but the but and and of it that camillo and he nor that of nor and it and the will so not you and to my i when the to\n\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 349ms/step - accuracy: 0.9864 - loss: 0.0550 - perplexity: 1.0565\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"transformer.save_weights('Shakespeare_decoder.weights.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T07:06:59.584011Z","iopub.execute_input":"2025-11-12T07:06:59.584545Z","iopub.status.idle":"2025-11-12T07:06:59.948656Z","shell.execute_reply.started":"2025-11-12T07:06:59.584522Z","shell.execute_reply":"2025-11-12T07:06:59.948008Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"out = transformer(vectorizer(['Thou shall not']))\npreds = tf.argmax(out[0])\nfor i,j in enumerate(preds): \n    if i > 50:\n        break\n    next_word = vectorizer.get_vocabulary()[i]\n    print(next_word)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T07:07:06.666470Z","iopub.execute_input":"2025-11-12T07:07:06.666776Z","iopub.status.idle":"2025-11-12T07:07:07.880030Z","shell.execute_reply.started":"2025-11-12T07:07:06.666756Z","shell.execute_reply":"2025-11-12T07:07:07.879223Z"}},"outputs":[{"name":"stdout","text":"\n[UNK]\nthe\nand\nto\ni\nof\nmy\nyou\na\nthat\nin\nis\nnot\nfor\nwith\nit\nme\nyour\nbe\nhis\nhe\nthis\nbut\nhave\nas\nthou\nhim\nso\nwhat\nthy\nwill\nking\nby\nno\nwe\nall\nshall\nour\ndo\nif\nher\nare\nlord\nthee\nnow\non\ngood\nfrom\no\ncome\n","output_type":"stream"}],"execution_count":22}]}