{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4558742,"sourceType":"datasetVersion","datasetId":2660745}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport os\nfrom tensorflow.keras import layers\nimport string","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 64\nNUM_HEADS = 12\nNUM_BLOCKS = 12\nEMBED_DIM = 768\nDENSE_DIM = 3072\nDROPOUT_RATE = 0.1\nCHUNK_LENGTH = 150","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/the-bards-best-a-character-modeling-dataset/train.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = '<sos>' + df.values[0][0] + '<eos>'\ntext = re.sub(r'\\s+', ' ', str(text)).strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\nwords = text.split()\n\n# Count unique words\nunique_words = set(words)\nprint(f\"Total words: {len(words)}\")\nprint(f\"Unique words: {len(unique_words)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chunk_text_by_words(text, max_words, stride=None):\n    words = text.split()\n    if stride is None:\n        stride = max_words // 2\n    chunks = []\n    for i in range(0, len(words) - max_words, stride):\n        chunk = ' '.join(words[i:i + max_words])\n        chunks.append(chunk)\n    return chunks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chunks = chunk_text_by_words(text, CHUNK_LENGTH+1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"strip_chars = string.punctuation + \"Â¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(\n        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n\nvocab_size = 23850\nsequence_length = CHUNK_LENGTH+1\n\nvectorizer = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n    standardization=custom_standardization\n)\n\nvectorizer.adapt(chunks)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_dataset(chunks):\n    tokens = vectorizer(chunks)\n    tokens_inp = tokens[:,:CHUNK_LENGTH]\n    tokens_out = tokens[:,1:]\n    ds = tf.data.Dataset.from_tensor_slices((tokens_inp,tokens_out))\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.shuffle(1024).prefetch(16).cache()\n    return ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = make_dataset(chunks)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEmbedding(tf.keras.layers.Layer):\n    def __init__(self, sequence_length, vocab_size, output_dim):\n        super().__init__()\n        self.positional_embedding = tf.keras.layers.Embedding(input_dim = sequence_length, output_dim = output_dim, mask_zero=False)\n        self.token_embedding = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim= output_dim, mask_zero=True)\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embedding(inputs)\n        embedded_positions = self.positional_embedding(positions)\n        return embedded_tokens + embedded_positions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerDecoder(tf.keras.layers.Layer):\n    def __init__(self, num_heads, embed_dim, dense_dim, dropout_rate):\n        super().__init__()\n        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,\n                                                           key_dim=embed_dim//num_heads)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n        self.dense_proj = tf.keras.models.Sequential([\n            tf.keras.layers.Dense(dense_dim, activation='relu'),\n            tf.keras.layers.Dense(embed_dim)\n        ])\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n    def call(self, inputs):\n        attn_out = self.attention(query=inputs,\n                            key=inputs,\n                            value=inputs,\n                            use_causal_mask=True)\n        norm1_out = self.layernorm1(attn_out+inputs)\n        drop1_out = self.dropout1(norm1_out)\n        dense_proj_out = self.dense_proj(drop1_out)\n        norm2_out = self.layernorm2(drop1_out+dense_proj_out)\n        drop2_out = self.dropout2(norm2_out)\n        return drop2_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = tf.keras.layers.Input(shape=(None,))\nembeddings = PositionalEmbedding(sequence_length, vocab_size, EMBED_DIM)(inputs)\nx = embeddings\nfor layer in range(NUM_BLOCKS):\n    x = TransformerDecoder(NUM_HEADS, EMBED_DIM, DENSE_DIM, DROPOUT_RATE)(x)\nx = tf.keras.layers.Dropout(0.5)(x)\noutput = tf.keras.layers.Dense(vocab_size,activation='softmax')(x)\ntransformer = tf.keras.models.Model(inputs, output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformer.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nopt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n\ntransformer.compile(loss = loss_fn,\n                    metrics = ['accuracy'],\n                    optimizer=opt)\ntransformer.fit(ds, epochs = 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}