{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4558742,"sourceType":"datasetVersion","datasetId":2660745}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport os\nfrom tensorflow.keras import layers\nimport string","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:58:22.525467Z","iopub.execute_input":"2025-11-07T07:58:22.526001Z","iopub.status.idle":"2025-11-07T07:58:22.529930Z","shell.execute_reply.started":"2025-11-07T07:58:22.525970Z","shell.execute_reply":"2025-11-07T07:58:22.529255Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"BATCH_SIZE = 64\nNUM_HEADS = 12\nNUM_BLOCKS = 12\nEMBED_DIM = 768\nDENSE_DIM = 3072\nDROPOUT_RATE = 0.1\nCHUNK_LENGTH = 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:58:22.531333Z","iopub.execute_input":"2025-11-07T07:58:22.531581Z","iopub.status.idle":"2025-11-07T07:58:22.548457Z","shell.execute_reply.started":"2025-11-07T07:58:22.531566Z","shell.execute_reply":"2025-11-07T07:58:22.547711Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/the-bards-best-a-character-modeling-dataset/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:58:22.549218Z","iopub.execute_input":"2025-11-07T07:58:22.549472Z","iopub.status.idle":"2025-11-07T07:58:22.574277Z","shell.execute_reply.started":"2025-11-07T07:58:22.549441Z","shell.execute_reply":"2025-11-07T07:58:22.573576Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"text = '<sos>' + df.values[0][0] + '<eos>'\ntext = re.sub(r'\\s+', ' ', str(text)).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:58:22.574964Z","iopub.execute_input":"2025-11-07T07:58:22.575190Z","iopub.status.idle":"2025-11-07T07:58:22.642286Z","shell.execute_reply.started":"2025-11-07T07:58:22.575176Z","shell.execute_reply":"2025-11-07T07:58:22.641534Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def chunk_text_by_words(text, max_words):\n    words = text.split()\n    chunks = []\n    for i in range(0, len(words), max_words):\n        chunk = ' '.join(words[i:i + max_words])\n        chunks.append(chunk)\n    return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:58:22.644117Z","iopub.execute_input":"2025-11-07T07:58:22.644321Z","iopub.status.idle":"2025-11-07T07:58:22.655828Z","shell.execute_reply.started":"2025-11-07T07:58:22.644306Z","shell.execute_reply":"2025-11-07T07:58:22.655239Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"chunks = chunk_text_by_words(text, CHUNK_LENGTH+1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:58:22.656805Z","iopub.execute_input":"2025-11-07T07:58:22.657119Z","iopub.status.idle":"2025-11-07T07:58:22.694578Z","shell.execute_reply.started":"2025-11-07T07:58:22.657095Z","shell.execute_reply":"2025-11-07T07:58:22.693869Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"strip_chars = string.punctuation + \"Â¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(\n        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n\nvocab_size = 18000\nsequence_length = CHUNK_LENGTH+1\n\nvectorizer = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n)\n\nvectorizer.adapt(chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:58:22.695305Z","iopub.execute_input":"2025-11-07T07:58:22.695510Z","iopub.status.idle":"2025-11-07T07:58:22.803291Z","shell.execute_reply.started":"2025-11-07T07:58:22.695494Z","shell.execute_reply":"2025-11-07T07:58:22.802679Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def make_dataset(chunks):\n    tokens = vectorizer(chunks)\n    tokens_inp = tokens[:,:CHUNK_LENGTH]\n    tokens_out = tokens[:,1:]\n    ds = tf.data.Dataset.from_tensor_slices((tokens_inp,tokens_out))\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.shuffle(1024).prefetch(16).cache()\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:58:40.022580Z","iopub.execute_input":"2025-11-07T07:58:40.022875Z","iopub.status.idle":"2025-11-07T07:58:40.027273Z","shell.execute_reply.started":"2025-11-07T07:58:40.022856Z","shell.execute_reply":"2025-11-07T07:58:40.026645Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"ds = make_dataset(chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:58:44.498379Z","iopub.execute_input":"2025-11-07T07:58:44.498704Z","iopub.status.idle":"2025-11-07T07:58:44.571080Z","shell.execute_reply.started":"2025-11-07T07:58:44.498686Z","shell.execute_reply":"2025-11-07T07:58:44.570278Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class PositionalEmbedding(tf.keras.layers.Layer):\n    def __init__(self, sequence_length, vocab_size, output_dim):\n        super().__init__()\n        self.positional_embedding(input_dim = sequence_length, output_dim = output_dim, mask_zero=False)\n        self.token_embedding(input_dim = vocab_size, output_dim= output_dim, mask_zero=True)\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embedding(inputs)\n        embedded_positions = self.position_embedding(positions)\n        return embedded_tokens + embedded_positions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T07:58:22.894909Z","iopub.status.idle":"2025-11-07T07:58:22.895146Z","shell.execute_reply.started":"2025-11-07T07:58:22.895040Z","shell.execute_reply":"2025-11-07T07:58:22.895050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerDecoder(tf.keras.layers.Layer):\n    def __init__(self, num_heads, embed_dim, dense_dim, dropout_rate):\n        super().__init__()\n        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,\n                                                           key_dim=embed_dim/num_heads)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n        self.dense_proj = tf.keras.models.Sequential([\n            tf.keras.layers.Dense(dense_dim, activation='relu'),\n            tf.keras.layers.Dense(embed_dim, activation='relu')\n        ])\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n    def call(self, inputs):\n        attn_out = self.attention(query=inputs,\n                            key=inputs,\n                            value=inputs,\n                            mask=None,\n                            use_causal_mask=True)\n        norm1_out = self.layernorm1(attn_out+inputs)\n        drop1_out = self.dropout1(norm_out)\n        dense_proj_out = dense_proj(drop_out)\n        norm2_out = self.layernorm2(drop1_out+dense_proj_out)\n        drop2_out = self.dropout2(norm2_out)\n        return drop2_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = tf.keras.layers.Input(shape=(None,))\nembeddings = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\nx = embeddings\nfor layer in range(NUM_BLOCKS):\n    x = TransformerDecoder(NUM_HEADS, EMBED_DIM, DENSE_DIM, DROPOUT_RATE)(x)\nx = tf.keras.layers.Dropout(0.5)(x)\noutput = tf.keras.layers.Dense(vocab_size,activation='softmax')(x)\ntransformer = tf.keras.models.Model(inputs, output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformer.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}